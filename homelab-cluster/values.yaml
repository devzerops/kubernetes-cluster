global:
  podSecurity:
    enforce: baseline
    audit: baseline
    warn: baseline
  podSecurityExemptions:
    namespaces:
      - kube-system
      - cert-manager
      - cilium
      - elastic-stack
      - elastic-system
      - argocd
      - monitoring
      - rook-ceph
      - vault

cert-manager:
  namespace: cert-manager

cilium:
  version: 1.16.3
  namespace: kube-system

  cni:
    exclusive: false

  socketLB:
    hostNamespaceOnly: true

  mtls:
    mode: DISABLE

  ipam:
    mode: kubernetes

  kubeProxyReplacement: true

  securityContext:
    capabilities:
      ciliumAgent:
        - CHOWN
        - KILL
        - NET_ADMIN
        - NET_RAW
        - IPC_LOCK
        - SYS_ADMIN
        - SYS_RESOURCE
        - DAC_OVERRIDE
        - FOWNER
        - SETGID
        - SETUID
      cleanCiliumState:
        - NET_ADMIN
        - SYS_ADMIN
        - SYS_RESOURCE

  cgroup:
    autoMount:
      enabled: false
    hostRoot: /sys/fs/cgroup

  k8sServiceHost: localhost
  k8sServicePort: 7445

  k8sClientRateLimit:
    qps: 5
    burst: 10

  l2announcements:
    enabled: true
    leaseDuration: 15s
    leaseRenewDeadline: 5s
    leaseRetryPeriod: 2s
    algorithm: maglev

  externalIPs:
    enabled: true

  hubble:
    ui:
      enabled: true
    relay:
      enabled: true

eck-stack:
  namespace: elastic-stack
  podSecurity:
    enforce: privileged
  eck-elasticsearch:
    nodeSets:
    - name: default
      count: 3
      config:
        # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
        # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
        # and leave node.store.allow_mmap unset.
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
        #
        node.store.allow_mmap: false
      podTemplate:
        spec:
          containers:
          - name: elasticsearch
            resources:
              limits:
                memory: 2Gi
              requests:
                memory: 2Gi
  eck-kibana:
    enabled: true
    fullnameOverride: kibana
    spec:
      count: 1
      elasticsearchRef:
        name: elasticsearch
      enterpriseSearchRef:
        name: enterprise-search

  eck-beats:
    enabled: true
    spec:
      type: filebeat
      daemonSet: null
      config:
        filebeat.inputs:
        - type: log
          paths:
            - /data/logstash-tutorial.log
        processors:
        - add_host_metadata: {}
        - add_cloud_metadata: {}
        output.logstash:
          # This needs to be {{logstash-name}}-ls-beats:5044
          hosts: ["logstash-ls-beats-ls-beats:5044"]
      deployment:
        podTemplate:
          spec:
            automountServiceAccountToken: true
            initContainers:
              - name: download-tutorial
                image: curlimages/curl
                command: ["/bin/sh"]
                args: ["-c", "curl -L https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz | gunzip -c > /data/logstash-tutorial.log"]
                volumeMounts:
                  - name: data
                    mountPath: /data
            containers:
              - name: filebeat
                securityContext:
                  runAsUser: 1000
                volumeMounts:
                  - name: data
                    mountPath: /data
                  - name: beat-data
                    mountPath: /usr/share/filebeat/data
            volumes:
              - name: data
                emptydir: {}
              - name: beat-data
                emptydir: {}
  eck-logstash:
    enabled: true
    # This is required to be able to set the logstash
    # output of beats in a consistent manner.
    fullnameOverride: "logstash-ls-beats"
    elasticsearchRefs:
      # This clusterName is required to match the environment variables
      # used in the below config.string output section.
      - clusterName: eck
        name: elasticsearch
    pipelines:
      - pipeline.id: main
        config.string: |
          input {
            beats {
              port => 5044
            }
          }
          filter {
            grok {
              match => { "message" => "%{HTTPD_COMMONLOG}"}
            }
            geoip {
              source => "[source][address]"
              target => "[source]"
            }
          }
          output {
            elasticsearch {
              hosts => [ "${ECK_ES_HOSTS}" ]
              user => "${ECK_ES_USER}"
              password => "${ECK_ES_PASSWORD}"
              ssl_certificate_authorities => "${ECK_ES_SSL_CERTIFICATE_AUTHORITY}"
            }
          }
    services:
      - name: beats
        service:
          spec:
            type: ClusterIP
            ports:
              - port: 5044
                name: "filebeat"
                protocol: TCP
                targetPort: 5044
  eck-enterprise-search:
    enabled: true

    # Name of the Enterprise Search instance.
    #
    fullnameOverride: enterprise-search

    elasticsearchRef:
      name: elasticsearch

eck-operator:
  namespace: elastic-system
  installCRDs: false

argo-cd:
  namespace: argocd
  podSecurity:
  enforce: privileged
  configs:
    params:
      server.insecure: true
  redis-ha:
    enabled: true

  controller:
    replicas: 1

  server:
    replicas: 2

  repoServer:
    replicas: 2

  applicationSet:
    replicas: 2
  

grafana:
  namespace: monitoring
  podSecurity:
  enforce: privileged

prometheus:
  namespace: monitoring
  podSecurity:
  enforce: privileged

  cephClusterSpec:
    mon:
      count: 3
    mgr:
      count: 1
    dashboard:
      enabled: true
    storage:
      useAllNodes: true
      useAllDevices: true
    
  cephBlockPools:
    - name: replicapool
      spec:
        failureDomain: host
        replicated:
          size: 3
      storageClass:
        enabled: true
        name: rook-ceph-block
        isDefault: true
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        
  cephFileSystems:
    - name: myfs
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          - name: replicated
            replicated:
              size: 3
        metadataServer:
          activeCount: 1
          activeStandby: true
      storageClass:
        enabled: true
        name: rook-cephfs
        
  cephObjectStores:
    - name: my-store
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
        gateway:
          port: 80
          instances: 1
      storageClass:
        enabled: true
        name: rook-ceph-bucket

vault:
  namespace: vault